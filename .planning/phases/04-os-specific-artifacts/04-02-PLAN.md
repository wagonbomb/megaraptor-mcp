---
phase: 04-os-specific-artifacts
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/integration/test_os_artifacts_windows.py
  - tests/conftest.py
autonomous: true

must_haves:
  truths:
    - "Windows artifact tests exist with proper skipif guards"
    - "Tests skip gracefully when no Windows target available"
    - "Tests would run and validate when Windows target becomes available"
    - "has_windows_target() helper function available in conftest"
  artifacts:
    - path: "tests/integration/test_os_artifacts_windows.py"
      provides: "Windows artifact collection tests with skip guards"
      contains: "test_windows_system_services"
    - path: "tests/conftest.py"
      provides: "has_windows_target() helper function"
      contains: "def has_windows_target"
  key_links:
    - from: "tests/integration/test_os_artifacts_windows.py"
      to: "tests/conftest.py"
      via: "pytest.mark.skipif(not has_windows_target())"
      pattern: "skipif.*has_windows_target"
    - from: "tests/integration/test_os_artifacts_windows.py"
      to: "tests/integration/schemas/os_artifacts.py"
      via: "validate with Windows schemas"
      pattern: "WINDOWS_SYSTEM_SERVICES_SCHEMA|WINDOWS_REGISTRY_USERASSIST_SCHEMA"
---

<objective>
Implement Windows artifact collection tests with proper skip guards for when Windows targets are unavailable.

Purpose: Validate OSART-02 (Windows.System.Services), OSART-03 (Windows registry artifacts), and OSART-05 (OS-specific validation) with tests that skip gracefully on Linux-only environments.
Output: Windows artifact tests that skip when no Windows target available, run when Windows target becomes available.
</objective>

<execution_context>
@C:\Users\Meow\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Meow\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-os-specific-artifacts/04-RESEARCH.md

# Existing patterns to follow
@tests/integration/test_smoke_artifacts.py
@tests/conftest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add has_windows_target() helper to conftest.py</name>
  <files>tests/conftest.py</files>
  <action>
Add a helper function to check for Windows target availability. This will be used by skipif decorators.

Add near the other `has_*` helper functions (after `has_velociraptor_configs()`):

```python
def has_windows_target() -> bool:
    """Check if a Windows test target is available.

    Checks by attempting target discovery. Returns True only if
    at least one Windows client is enrolled in the test Velociraptor.

    Note: This check is relatively expensive as it requires Docker
    infrastructure and gRPC connection. Use sparingly (module-level skipif).
    """
    if not has_docker() or not has_velociraptor_configs():
        return False

    if not is_velociraptor_running():
        return False

    try:
        from megaraptor_mcp.client import VelociraptorClient
        from megaraptor_mcp.config import VelociraptorConfig

        config = VelociraptorConfig.from_config_file(str(API_CLIENT_CONFIG))
        client = VelociraptorClient(config)
        client.connect()

        try:
            # Query for Windows clients
            result = client.query(
                "SELECT client_id FROM clients() WHERE os_info.system =~ 'windows'"
            )
            return len(result) > 0
        finally:
            client.close()
    except Exception:
        return False


# Skip decorator for Windows-required tests
skip_no_windows_target = pytest.mark.skipif(
    not has_windows_target(),
    reason="No Windows target available (Windows VM or host required)"
)
```

This follows the pattern of skip_no_docker and skip_no_configs.
  </action>
  <verify>
Run: `python -c "from tests.conftest import has_windows_target; print(f'has_windows_target={has_windows_target()}')"`
Should print: has_windows_target=False (since we only have Linux container)
  </verify>
  <done>has_windows_target() helper and skip_no_windows_target decorator available in conftest</done>
</task>

<task type="auto">
  <name>Task 2: Create Windows artifact tests with skip guards</name>
  <files>tests/integration/test_os_artifacts_windows.py</files>
  <action>
Create new file tests/integration/test_os_artifacts_windows.py with tests that skip when no Windows target available:

```python
"""Windows OS-specific artifact collection tests.

Validates:
- OSART-02: Windows.System.Services artifact collection works
- OSART-03: Windows registry artifact validation (UserAssist)
- OSART-05: OS-specific validation schemas for complex artifact types

These tests require a Windows target (VM or Windows host with Velociraptor client).
Tests skip gracefully when no Windows target is available.

To enable Windows testing:
1. Set up Windows VM with Velociraptor client
2. Enroll Windows client with test server
3. Run tests - they will auto-detect Windows target
"""

import pytest
from jsonschema import validate, ValidationError
from pytest_check import check

from tests.conftest import has_windows_target
from tests.integration.helpers.wait_helpers import wait_for_flow_completion
from tests.integration.schemas.os_artifacts import (
    WINDOWS_SYSTEM_SERVICES_SCHEMA,
    WINDOWS_REGISTRY_USERASSIST_SCHEMA,
)


# Module-level check - all tests in this file require Windows target
pytestmark = [
    pytest.mark.integration,
    pytest.mark.timeout(60),
]


@pytest.mark.skipif(not has_windows_target(), reason="No Windows target available")
class TestWindowsArtifacts:
    """Windows-specific artifact collection tests.

    All tests in this class require a Windows target.
    They skip gracefully when only Linux targets are available.
    """

    def test_windows_system_services(self, velociraptor_client, target_registry):
        """Test Windows.System.Services artifact collection.

        Validates OSART-02: Windows.System.Services artifact collection
        returns service data from Windows targets.

        This test:
        1. Selects Windows target via TargetRegistry
        2. Schedules Windows.System.Services artifact collection
        3. Waits for flow completion
        4. Validates results against JSON schema
        5. Verifies critical service fields present
        """
        # Get Windows target
        target = target_registry.get_by_artifact("Windows.System.Services")
        if not target:
            pytest.skip("No Windows target with service capability")

        assert target.os_type == "windows", \
            f"Expected Windows target, got {target.os_type}"

        client_id = target.client_id

        # Schedule artifact collection
        vql = f"""
        SELECT collect_client(
            client_id='{client_id}',
            artifacts=['Windows.System.Services'],
            timeout=30
        ) AS collection
        FROM scope()
        """

        result = velociraptor_client.query(vql)

        # Validate collection was scheduled
        with check:
            assert len(result) > 0, "collect_client returned no results"

        collection = result[0].get("collection", {})
        flow_id = collection.get("flow_id")

        if not flow_id:
            pytest.fail("No flow_id returned from collect_client")

        # Wait for flow completion
        try:
            wait_for_flow_completion(
                velociraptor_client,
                client_id,
                flow_id,
                timeout=30
            )
        except TimeoutError:
            pytest.fail("Windows.System.Services collection did not complete in 30s")

        # Get flow results
        results_vql = f"""
        SELECT * FROM source(
            client_id='{client_id}',
            flow_id='{flow_id}',
            artifact='Windows.System.Services'
        )
        """
        results = velociraptor_client.query(results_vql)

        # Validate results against JSON schema (OSART-05)
        try:
            validate(instance=results, schema=WINDOWS_SYSTEM_SERVICES_SCHEMA)
        except ValidationError as e:
            pytest.fail(f"Schema validation failed: {e.message}")

        # Validate we got service data
        with check:
            assert len(results) > 0, "Windows.System.Services returned no services"

        if results:
            service = results[0]

            # Verify critical fields present
            name_found = any(k in service for k in ["Name", "name", "ServiceName"])
            state_found = any(k in service for k in ["State", "state", "Status"])

            with check:
                assert name_found, \
                    f"Missing service name field. Available: {list(service.keys())}"
            with check:
                assert state_found, \
                    f"Missing service state field. Available: {list(service.keys())}"

            # Validate Name field type
            name_key = next((k for k in ["Name", "name", "ServiceName"] if k in service), None)
            if name_key:
                with check:
                    assert isinstance(service[name_key], str), \
                        f"Service name should be string, got {type(service[name_key])}"

        # Windows should have many services
        with check:
            assert len(results) >= 10, \
                f"Expected at least 10 services, got {len(results)}"


    def test_windows_registry_userassist(self, velociraptor_client, target_registry):
        """Test Windows.Registry.UserAssist artifact collection.

        Validates OSART-03: Windows registry artifact validation works.

        UserAssist registry keys track program execution via Explorer.
        The artifact:
        - Parses binary GUID structures
        - Decodes ROT13-encoded application names
        - Extracts execution timestamps and counts

        Note: UserAssist may return empty results on clean/new systems
        where no Explorer-launched programs have been tracked yet.
        Empty results are valid and should not fail the test.
        """
        # Get Windows target with registry capability
        target = target_registry.get_by_capability("windows_registry")
        if not target:
            pytest.skip("No Windows target with registry capability")

        assert target.os_type == "windows", \
            f"Expected Windows target, got {target.os_type}"

        client_id = target.client_id

        # Schedule artifact collection
        vql = f"""
        SELECT collect_client(
            client_id='{client_id}',
            artifacts=['Windows.Registry.UserAssist'],
            timeout=30
        ) AS collection
        FROM scope()
        """

        result = velociraptor_client.query(vql)

        # Validate collection was scheduled
        with check:
            assert len(result) > 0, "collect_client returned no results"

        collection = result[0].get("collection", {})
        flow_id = collection.get("flow_id")

        if not flow_id:
            pytest.fail("No flow_id returned from collect_client")

        # Wait for flow completion
        try:
            wait_for_flow_completion(
                velociraptor_client,
                client_id,
                flow_id,
                timeout=30
            )
        except TimeoutError:
            pytest.fail("Windows.Registry.UserAssist collection did not complete in 30s")

        # Get flow results
        results_vql = f"""
        SELECT * FROM source(
            client_id='{client_id}',
            flow_id='{flow_id}',
            artifact='Windows.Registry.UserAssist'
        )
        """
        results = velociraptor_client.query(results_vql)

        # Validate results against JSON schema (OSART-05)
        # Note: Empty results are valid for UserAssist
        try:
            validate(instance=results, schema=WINDOWS_REGISTRY_USERASSIST_SCHEMA)
        except ValidationError as e:
            pytest.fail(f"Schema validation failed: {e.message}")

        # If we have results, validate structure
        if len(results) > 0:
            entry = results[0]

            # Verify ROT13-decoded name is present
            # If Name is still ROT13-encoded, parsing failed
            name_found = any(k in entry for k in ["Name", "name", "AppName"])

            with check:
                assert name_found, \
                    f"Missing Name field in UserAssist. Available: {list(entry.keys())}"

            # Verify execution metadata present
            timestamp_found = any(k in entry for k in [
                "LastExecution", "LastExecutionTime", "last_execution"
            ])
            count_found = any(k in entry for k in [
                "NumberOfExecutions", "RunCount", "run_count"
            ])

            with check:
                assert timestamp_found, \
                    f"Missing execution timestamp. Available: {list(entry.keys())}"
            with check:
                assert count_found, \
                    f"Missing execution count. Available: {list(entry.keys())}"

            # Validate Name is not ROT13-encoded (basic sanity check)
            # ROT13-encoded Windows paths typically start with ".N..." (rotated "C:\...")
            name_key = next((k for k in ["Name", "name", "AppName"] if k in entry), None)
            if name_key:
                name_value = entry[name_key]
                if isinstance(name_value, str) and len(name_value) > 3:
                    # ROT13 of "C:\" is ".N:"
                    with check:
                        assert not name_value.startswith(".N:"), \
                            f"Name appears to be ROT13-encoded: {name_value[:20]}..."
        else:
            # Empty results are valid - just log it
            print("Note: Windows.Registry.UserAssist returned no entries (normal on clean systems)")


    def test_windows_services_capability_check(self, target_registry):
        """Test that Windows targets have windows_services capability.

        Validates that TargetRegistry correctly assigns capabilities
        to Windows targets.
        """
        windows_target = target_registry.get_by_os("windows")
        if not windows_target:
            pytest.skip("No Windows target available")

        # Windows targets should have service capability
        with check:
            assert "windows_services" in windows_target.capabilities, \
                f"Windows target missing windows_services capability. Has: {windows_target.capabilities}"

        # Windows targets should also have registry capability
        with check:
            assert "windows_registry" in windows_target.capabilities, \
                f"Windows target missing windows_registry capability. Has: {windows_target.capabilities}"
```

Key design decisions:
- Class-level @pytest.mark.skipif ensures all tests skip together
- Tests validate against schemas from os_artifacts.py (created in Plan 01)
- UserAssist test handles empty results gracefully (valid on clean systems)
- ROT13 decoding validation ensures artifact parsing works correctly
- Follows established patterns from test_smoke_artifacts.py
  </action>
  <verify>
Run: `pytest tests/integration/test_os_artifacts_windows.py -v --tb=short`
All tests should SKIP with reason "No Windows target available"
  </verify>
  <done>Windows artifact tests exist and skip gracefully when no Windows target available</done>
</task>

<task type="auto">
  <name>Task 3: Add windows marker to pytest configuration</name>
  <files>tests/conftest.py</files>
  <action>
Add windows marker registration in pytest_configure() function.

Update pytest_configure() to include:

```python
def pytest_configure(config):
    """Register custom markers."""
    config.addinivalue_line("markers", "unit: Unit tests (no external dependencies)")
    config.addinivalue_line("markers", "integration: Requires Docker infrastructure")
    config.addinivalue_line("markers", "slow: Long-running tests")
    config.addinivalue_line("markers", "smoke: Smoke tests (quick validation)")
    config.addinivalue_line("markers", "windows: Requires Windows target")  # Add this
```

This allows filtering Windows tests with `pytest -m windows` or excluding them with `pytest -m "not windows"`.
  </action>
  <verify>
Run: `pytest --markers | grep windows`
Should show: windows: Requires Windows target
  </verify>
  <done>Windows marker registered for test filtering</done>
</task>

</tasks>

<verification>
1. Helper function: `python -c "from tests.conftest import has_windows_target; print(has_windows_target())"`
2. Tests skip correctly: `pytest tests/integration/test_os_artifacts_windows.py -v --tb=short`
3. Marker registered: `pytest --markers | grep windows`
4. No regressions: `pytest tests/integration/ -v --tb=short -x`
</verification>

<success_criteria>
- [ ] has_windows_target() function exists and returns False (no Windows target)
- [ ] skip_no_windows_target decorator available for other tests
- [ ] Windows artifact tests skip with clear reason message
- [ ] Windows marker registered for test filtering
- [ ] All existing tests continue to pass (no regressions)
- [ ] When Windows target becomes available, tests would run (code paths valid)
</success_criteria>

<output>
After completion, create `.planning/phases/04-os-specific-artifacts/04-02-SUMMARY.md`
</output>
