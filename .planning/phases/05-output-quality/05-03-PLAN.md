---
phase: 05-output-quality
plan: 03
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - tests/integration/test_output_quality.py
  - tests/fixtures/baselines/metadata.json
autonomous: true

must_haves:
  truths:
    - "Completeness validation tests verify all expected fields are present"
    - "VQL correctness tests compare results against known-good baselines"
    - "NIST CFTT false positive definition documented for VQL artifacts"
  artifacts:
    - path: "tests/integration/test_output_quality.py"
      provides: "Completeness and correctness tests"
      contains: "test_artifact_completeness"
    - path: "tests/fixtures/baselines/metadata.json"
      provides: "False positive definition per artifact"
      contains: "false_positive_definition"
  key_links:
    - from: "tests/integration/test_output_quality.py"
      to: "tests/integration/schemas/os_artifacts.py"
      via: "jsonschema validation for completeness"
      pattern: "from tests.integration.schemas"
    - from: "tests/integration/test_output_quality.py"
      to: "tests/fixtures/baselines/"
      via: "load_baseline for correctness comparison"
      pattern: "load_baseline"
---

<objective>
Implement completeness validation and VQL correctness tests with NIST CFTT false positive definition

Purpose: Satisfy QUAL-03 (artifact completeness validation), QUAL-04 (VQL result correctness against baselines), and QUAL-06 (NIST CFTT false positive rate definition). These tests ensure forensic soundness by validating field presence and result accuracy.

Output: Additional tests in test_output_quality.py for completeness and correctness, plus NIST CFTT false positive definitions in baseline metadata.
</objective>

<execution_context>
@C:\Users\Meow\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Meow\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-output-quality/05-RESEARCH.md
@.planning/phases/05-output-quality/05-01-PLAN.md
@.planning/phases/05-output-quality/05-02-PLAN.md

# Existing patterns
@tests/integration/test_os_artifacts_linux.py
@tests/integration/schemas/os_artifacts.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add NIST CFTT false positive definitions to baseline metadata</name>
  <files>
    tests/fixtures/baselines/metadata.json
  </files>
  <action>
Update `tests/fixtures/baselines/metadata.json` to include NIST CFTT false positive definitions for each artifact.

Add to each baseline entry in the "baselines" object:
```json
"false_positive_definition": {
  "description": "[What constitutes a false positive for this artifact]",
  "examples": ["[Example false positive case]"],
  "target_rate": "<1%",
  "rationale": "[Why this definition applies]"
}
```

Specific definitions per research (QUAL-06):

**For Linux.Sys.Users:**
```json
"false_positive_definition": {
  "description": "User returned that does not exist in /etc/passwd",
  "examples": [
    "User 'ghost' returned but not in /etc/passwd",
    "Duplicate user entries for same UID"
  ],
  "target_rate": "<1%",
  "rationale": "VQL is deterministic - false positives indicate VQL bug or parsing error"
}
```

**For Generic.Client.Info:**
```json
"false_positive_definition": {
  "description": "Client metadata returned that does not match actual client",
  "examples": [
    "Wrong hostname returned",
    "Incorrect OS reported",
    "Mismatched client_id"
  ],
  "target_rate": "<1%",
  "rationale": "VQL queries system APIs directly - false positives indicate API or parsing issues"
}
```

Also add top-level NIST CFTT documentation:
```json
"nist_cftt": {
  "standard": "NIST Computer Forensics Tool Testing (CFTT)",
  "requirement": "False positive rate < 1%",
  "applicability": "VQL is deterministic - target 0% for standard queries",
  "notes": "False positive definitions are artifact-specific. For deterministic VQL, any false positive indicates a bug."
}
```

Read existing metadata.json first to preserve structure, then add new fields.
  </action>
  <verify>
    - `python -c "import json; m=json.load(open('tests/fixtures/baselines/metadata.json')); print('nist_cftt' in m)"`
    - `python -c "import json; m=json.load(open('tests/fixtures/baselines/metadata.json')); print('false_positive_definition' in m['baselines']['Linux.Sys.Users'])"`
  </verify>
  <done>
    - Each baseline has false_positive_definition with description, examples, target_rate
    - nist_cftt top-level section documents the standard and applicability
    - QUAL-06 requirement addressed (false positive rate definition)
  </done>
</task>

<task type="auto">
  <name>Task 2: Add completeness and correctness tests to test_output_quality.py</name>
  <files>
    tests/integration/test_output_quality.py
  </files>
  <action>
Add two new test classes to `tests/integration/test_output_quality.py`:

1. **TestArtifactCompleteness** (QUAL-03):
```python
@pytest.mark.integration
@pytest.mark.timeout(90)
class TestArtifactCompleteness:
    """QUAL-03: Artifact completeness validation tests."""

    @pytest.mark.parametrize("artifact_name,required_fields", [
        ("Linux.Sys.Users", ["User"]),
        ("Generic.Client.Info", []),  # Has flexible structure
    ])
    def test_artifact_completeness_validation(
        self, artifact_name, required_fields, velociraptor_client, target_registry
    ):
        """Validate all expected fields are present in artifact results.

        Tests QUAL-03: Artifact completeness validation ensures all
        expected fields present.
        """
        target = target_registry.get_by_artifact(artifact_name)
        if not target:
            pytest.skip(f"No target available for {artifact_name}")

        client_id = target.client_id

        # Collect artifact
        vql = f"""
        SELECT collect_client(
            client_id='{client_id}',
            artifacts=['{artifact_name}'],
            timeout=30
        ) AS collection
        FROM scope()
        """
        result = velociraptor_client.query(vql)

        with check:
            assert len(result) > 0, "collect_client returned no results"

        collection = result[0].get("collection", {})
        flow_id = collection.get("flow_id")

        if not flow_id:
            pytest.fail("No flow_id returned")

        # Wait for completion
        try:
            wait_for_flow_completion(
                velociraptor_client, client_id, flow_id, timeout=30
            )
        except TimeoutError:
            pytest.fail("Collection did not complete in 30s")

        # Get results - handle artifacts with and without sub-sources
        if artifact_name == "Generic.Client.Info":
            results_vql = f"""
            SELECT * FROM source(
                client_id='{client_id}',
                flow_id='{flow_id}',
                artifact='Generic.Client.Info/BasicInformation'
            )
            """
        else:
            results_vql = f"""
            SELECT * FROM source(
                client_id='{client_id}',
                flow_id='{flow_id}',
                artifact='{artifact_name}'
            )
            """
        results = velociraptor_client.query(results_vql)

        # Validate results not empty
        with check:
            assert len(results) > 0, f"{artifact_name} returned no results"

        if results:
            # Validate required fields present in all rows
            for field in required_fields:
                field_present = all(
                    any(k.lower() == field.lower() for k in r.keys())
                    for r in results
                )
                with check:
                    assert field_present, (
                        f"Missing required field '{field}' in {artifact_name}"
                    )

            # Validate no empty required fields
            for field in required_fields:
                for r in results:
                    # Find field with case-insensitive match
                    actual_key = next(
                        (k for k in r.keys() if k.lower() == field.lower()),
                        None
                    )
                    if actual_key:
                        with check:
                            assert r[actual_key] is not None, (
                                f"Required field '{field}' is None"
                            )
                            if isinstance(r[actual_key], str):
                                assert len(r[actual_key]) > 0, (
                                    f"Required field '{field}' is empty string"
                                )


    def test_completeness_field_count_reasonable(
        self, velociraptor_client, target_registry
    ):
        """Verify artifacts return reasonable number of fields.

        Completeness also means getting expected field count, not just
        required fields.
        """
        target = target_registry.get_by_artifact("Linux.Sys.Users")
        if not target:
            pytest.skip("No Linux target available")

        client_id = target.client_id

        # Quick collection
        vql = f"""
        SELECT collect_client(
            client_id='{client_id}',
            artifacts=['Linux.Sys.Users'],
            timeout=30
        ) AS collection
        FROM scope()
        """
        result = velociraptor_client.query(vql)
        collection = result[0].get("collection", {})
        flow_id = collection.get("flow_id")

        if not flow_id:
            pytest.fail("No flow_id returned")

        wait_for_flow_completion(velociraptor_client, client_id, flow_id, timeout=30)

        results_vql = f"""
        SELECT * FROM source(
            client_id='{client_id}',
            flow_id='{flow_id}',
            artifact='Linux.Sys.Users'
        )
        """
        results = velociraptor_client.query(results_vql)

        if results:
            # Linux.Sys.Users should have at least User, Uid, Gid
            # (typically also Homedir, Shell, Description)
            field_count = len(results[0].keys())
            with check:
                assert field_count >= 3, (
                    f"Expected at least 3 fields, got {field_count}: "
                    f"{list(results[0].keys())}"
                )
```

2. **TestVQLCorrectness** (QUAL-04):
```python
@pytest.mark.integration
@pytest.mark.timeout(120)
class TestVQLCorrectness:
    """QUAL-04: VQL result correctness against known-good baselines."""

    def test_vql_correctness_linux_sys_users(
        self, velociraptor_client, target_registry
    ):
        """Validate Linux.Sys.Users results match baseline structure.

        Compares actual results against baseline for:
        - Field presence (critical fields exist)
        - Value types (strings vs integers)
        - Result count (reasonable magnitude)
        """
        target = target_registry.get_by_artifact("Linux.Sys.Users")
        if not target:
            pytest.skip("No Linux target available")

        client_id = target.client_id

        # Collect artifact
        vql = f"""
        SELECT collect_client(
            client_id='{client_id}',
            artifacts=['Linux.Sys.Users'],
            timeout=30
        ) AS collection
        FROM scope()
        """
        result = velociraptor_client.query(vql)
        collection = result[0].get("collection", {})
        flow_id = collection.get("flow_id")

        if not flow_id:
            pytest.fail("No flow_id returned")

        wait_for_flow_completion(velociraptor_client, client_id, flow_id, timeout=30)

        results_vql = f"""
        SELECT * FROM source(
            client_id='{client_id}',
            flow_id='{flow_id}',
            artifact='Linux.Sys.Users'
        )
        """
        actual_results = velociraptor_client.query(results_vql)

        # Load baseline
        baseline = load_baseline("Linux.Sys.Users")

        if not baseline:
            pytest.skip(
                "Baseline not populated. First results:\n"
                f"{actual_results[:2] if actual_results else 'No results'}"
            )

        # Load critical fields from metadata
        metadata = load_baseline_metadata()
        critical_fields = metadata.get("baselines", {}).get(
            "Linux.Sys.Users", {}
        ).get("critical_fields", ["User", "Uid", "Gid"])

        # Validate critical fields present
        if actual_results:
            actual_fields = set(actual_results[0].keys())
            for field in critical_fields:
                # Case-insensitive check
                field_found = any(
                    k.lower() == field.lower() for k in actual_fields
                )
                with check:
                    assert field_found, (
                        f"Missing critical field: {field}\n"
                        f"Available: {actual_fields}"
                    )

        # Validate result count in reasonable range
        # (Â±50% of baseline, as per research Pattern 4)
        if baseline:
            baseline_count = len(baseline)
            actual_count = len(actual_results)

            # Allow 50% variance
            with check:
                assert actual_count >= baseline_count * 0.5, (
                    f"Result count too low: {actual_count} vs baseline {baseline_count}"
                )
            with check:
                assert actual_count <= baseline_count * 2.0, (
                    f"Result count too high: {actual_count} vs baseline {baseline_count}"
                )

    def test_vql_correctness_no_false_positives(
        self, velociraptor_client, target_registry
    ):
        """Validate VQL results contain no obvious false positives.

        For Linux.Sys.Users: All returned users should have valid structure.
        This tests QUAL-06: NIST CFTT false positive rate < 1%.
        """
        target = target_registry.get_by_artifact("Linux.Sys.Users")
        if not target:
            pytest.skip("No Linux target available")

        client_id = target.client_id

        # Collect artifact
        vql = f"""
        SELECT collect_client(
            client_id='{client_id}',
            artifacts=['Linux.Sys.Users'],
            timeout=30
        ) AS collection
        FROM scope()
        """
        result = velociraptor_client.query(vql)
        collection = result[0].get("collection", {})
        flow_id = collection.get("flow_id")

        if not flow_id:
            pytest.fail("No flow_id returned")

        wait_for_flow_completion(velociraptor_client, client_id, flow_id, timeout=30)

        results_vql = f"""
        SELECT * FROM source(
            client_id='{client_id}',
            flow_id='{flow_id}',
            artifact='Linux.Sys.Users'
        )
        """
        results = velociraptor_client.query(results_vql)

        if not results:
            pytest.skip("No results to validate")

        # Count potential false positives
        false_positives = 0
        total_results = len(results)

        for r in results:
            # Get username field
            user = r.get("User") or r.get("user") or r.get("Username")

            # False positive indicators for user data:
            # 1. Empty or None username
            if not user:
                false_positives += 1
                continue

            # 2. Username contains invalid characters (basic check)
            if '\x00' in str(user):
                false_positives += 1
                continue

            # 3. UID is negative (invalid)
            uid = r.get("Uid") or r.get("uid") or r.get("UID")
            if uid is not None:
                try:
                    if int(uid) < 0:
                        false_positives += 1
                        continue
                except (ValueError, TypeError):
                    pass  # Non-numeric UID, not necessarily false positive

        # Calculate false positive rate
        fp_rate = (false_positives / total_results * 100) if total_results > 0 else 0

        # NIST CFTT requires < 1% false positive rate
        with check:
            assert fp_rate < 1.0, (
                f"False positive rate {fp_rate:.2f}% exceeds NIST CFTT threshold of 1%\n"
                f"False positives: {false_positives}/{total_results}"
            )

        # For deterministic VQL, we actually expect 0% false positives
        with check:
            assert false_positives == 0, (
                f"VQL is deterministic - expected 0 false positives, got {false_positives}"
            )
```

Add necessary imports at top of file:
```python
from tests.integration.helpers.baseline_helpers import (
    compute_forensic_hash,
    load_baseline,
    load_baseline_metadata,
    parse_velociraptor_timestamp,
)
```

If `load_baseline` not already imported, add it.
  </action>
  <verify>
    - `python -c "import tests.integration.test_output_quality; print('Module imports OK')"`
    - `grep "class TestArtifactCompleteness" tests/integration/test_output_quality.py`
    - `grep "class TestVQLCorrectness" tests/integration/test_output_quality.py`
    - `grep "NIST CFTT" tests/integration/test_output_quality.py`
  </verify>
  <done>
    - TestArtifactCompleteness class validates QUAL-03 (field presence)
    - TestVQLCorrectness class validates QUAL-04 (baseline comparison)
    - test_vql_correctness_no_false_positives validates QUAL-06 (<1% false positive rate)
    - Tests use parametrize for multiple artifacts where appropriate
  </done>
</task>

</tasks>

<verification>
Run verification:
```bash
# Check test classes exist
grep -E "class Test(ArtifactCompleteness|VQLCorrectness)" tests/integration/test_output_quality.py

# Check NIST CFTT definition in metadata
python -c "import json; m=json.load(open('tests/fixtures/baselines/metadata.json')); print('NIST CFTT:', m.get('nist_cftt', {}).get('requirement', 'NOT FOUND'))"

# Run unit tests (if any don't require live server)
pytest tests/integration/test_output_quality.py -v -k "not integration" --collect-only

# Run integration tests (require live server)
pytest tests/integration/test_output_quality.py -v -m integration --timeout=180
```
</verification>

<success_criteria>
- [ ] metadata.json contains false_positive_definition for each baseline
- [ ] metadata.json contains nist_cftt top-level section
- [ ] TestArtifactCompleteness validates QUAL-03 (field presence)
- [ ] TestVQLCorrectness validates QUAL-04 (baseline comparison)
- [ ] test_vql_correctness_no_false_positives validates QUAL-06 (false positive rate)
- [ ] All tests run (skip or pass based on baseline state)
</success_criteria>

<output>
After completion, create `.planning/phases/05-output-quality/05-03-SUMMARY.md`
</output>
