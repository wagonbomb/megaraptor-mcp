---
phase: 02-smoke-tests
plan: 06
type: execute
wave: 2
depends_on: ["02-05"]
files_modified:
  - tests/integration/test_smoke_artifacts.py
  - tests/integration/test_smoke_mcp_tools.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "Generic.Client.Info artifact collection returns valid metadata"
    - "Generic.System.Pslist artifact collection returns process list"
    - "All 35 MCP tool smoke tests pass or fail gracefully"
  artifacts:
    - path: "tests/integration/test_smoke_artifacts.py"
      provides: "Working artifact collection smoke tests"
      contains: "test_generic_client_info"
    - path: "tests/integration/test_smoke_mcp_tools.py"
      provides: "Passing MCP tool smoke tests"
      contains: "TOOL_SMOKE_INPUTS"
  key_links:
    - from: "test_smoke_artifacts.py"
      to: "velociraptor_client fixture"
      via: "direct VQL query"
      pattern: "velociraptor_client\\.query"
    - from: "test_smoke_mcp_tools.py"
      to: "invoke_mcp_tool helper"
      via: "async invocation"
      pattern: "invoke_mcp_tool"
---

<objective>
Fix artifact collection tests after MCP SDK migration to close Gaps 2 and 3.

Purpose: After migrating to FastMCP (02-05), verify artifact collection works correctly. The tests were showing empty results - this may have been a symptom of the broken tool registration, or may require VQL adjustments for the Velociraptor version.

Output: Passing artifact smoke tests (SMOKE-02, SMOKE-03) and all 35 MCP tool tests running.
</objective>

<execution_context>
@C:\Users\Meow\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Meow\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-smoke-tests/02-VERIFICATION.md
@.planning/phases/02-smoke-tests/02-05-SUMMARY.md
@tests/integration/test_smoke_artifacts.py
@tests/integration/test_smoke_mcp_tools.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Verify and fix artifact collection VQL</name>
  <files>tests/integration/test_smoke_artifacts.py</files>
  <action>
Debug why artifact collection returns empty results. The tests use direct VQL through velociraptor_client fixture, not MCP tools, so this is independent of the FastMCP migration.

**Diagnostic approach:**

1. First verify the fixture provides an enrolled client:
```python
# Add diagnostic output at start of test
print(f"Testing with client_id: {enrolled_client_id}")
```

2. Check if the artifact name exists on this Velociraptor version:
```python
# Query artifact availability
artifacts_vql = "SELECT name FROM artifact_definitions() WHERE name =~ 'Generic.Client.Info'"
available = velociraptor_client.query(artifacts_vql)
print(f"Available artifacts: {available}")
```

3. Check flow state after completion:
```python
# After wait_for_flow_completion, check flow details
flow_vql = f"SELECT * FROM flows(client_id='{enrolled_client_id}', flow_id='{flow_id}')"
flow_state = velociraptor_client.query(flow_vql)
print(f"Flow state: {flow_state}")
```

4. Try alternative result retrieval methods if source() returns empty:
```python
# Alternative 1: Query flow results directly
alt_vql = f"SELECT * FROM flow_results(client_id='{enrolled_client_id}', flow_id='{flow_id}')"

# Alternative 2: Check if artifact name in source() needs full path
source_vql = f"SELECT * FROM source(client_id='{enrolled_client_id}', flow_id='{flow_id}')"
# (without specifying artifact - get all sources)
```

**Likely fixes:**

- If artifact name differs: Update to use correct artifact name for this Velociraptor version
- If source() syntax differs: Adjust VQL to match Velociraptor version
- If timing issue: Increase wait time or add small delay after flow completion
- If Linux container doesn't support artifact: Mark test as xfail with reason

The key is diagnosing first, then applying the minimal fix.
  </action>
  <verify>
```bash
cd C:/Users/Meow/Documents/Projects/megaraptor-mcp
pytest tests/integration/test_smoke_artifacts.py::TestArtifactCollectionSmoke::test_generic_client_info -v -s 2>&1 | tail -30
```
Should show test passing or clear diagnostic output about why it fails.
  </verify>
  <done>test_generic_client_info passes OR has clear skip/xfail with documented reason</done>
</task>

<task type="auto">
  <name>Task 2: Fix Generic.System.Pslist test</name>
  <files>tests/integration/test_smoke_artifacts.py</files>
  <action>
Apply same diagnostic approach to Pslist test:

1. Check if Generic.System.Pslist is available on Linux:
   - Linux containers use different process listing than Windows
   - May need Linux.Sys.Pslist or similar artifact

2. If Pslist unavailable, find Linux equivalent:
```python
# Check available process artifacts
artifacts_vql = "SELECT name FROM artifact_definitions() WHERE name =~ 'Pslist' OR name =~ 'pslist'"
```

3. Update test to use correct artifact for the target OS:
```python
# Determine artifact based on OS
os_info_vql = f"SELECT os_info.system FROM clients(client_id='{enrolled_client_id}')"
os_info = velociraptor_client.query(os_info_vql)
target_os = os_info[0].get('system', 'linux').lower() if os_info else 'linux'

if 'linux' in target_os:
    artifact = 'Linux.Sys.Pslist'  # or whatever is available
else:
    artifact = 'Generic.System.Pslist'
```

4. If no process list artifact works on this container:
   - Mark test as pytest.mark.xfail with reason
   - Document that Phase 4 (OS-Specific) will properly handle this

The goal is either a passing test or a properly-marked skip/xfail, not a blocking failure.
  </action>
  <verify>
```bash
cd C:/Users/Meow/Documents/Projects/megaraptor-mcp
pytest tests/integration/test_smoke_artifacts.py::TestArtifactCollectionSmoke::test_generic_system_pslist -v -s 2>&1 | tail -30
```
  </verify>
  <done>test_generic_system_pslist passes OR has clear skip/xfail with documented reason</done>
</task>

<task type="auto">
  <name>Task 3: Verify all MCP tool smoke tests run</name>
  <files>tests/integration/test_smoke_mcp_tools.py</files>
  <action>
After FastMCP migration (02-05), run the full MCP tool smoke test suite:

1. Run the test suite:
```bash
pytest tests/integration/test_smoke_mcp_tools.py -v --tb=short 2>&1
```

2. Expected outcomes per tool category:
   - **Client tools (4):** Should pass if enrolled_client_id works
   - **Artifact tools (3):** Should pass (list/get work, collect may need client)
   - **Hunt tools (4):** Should pass (create returns flow_id, others may return empty)
   - **Flow tools (4):** May fail gracefully if no flows exist
   - **VQL tools (2-3):** Should pass (run_vql, vql_help)
   - **Deployment tools (18):** Expected to fail gracefully (no infrastructure)

3. Fix any tests that fail due to FastMCP migration issues:
   - If invoke_mcp_tool fails: Check import path in mcp_helpers.py
   - If tool not found: Check tool name matches function name

4. Ensure graceful failures for expected cases:
   - Deployment tools should return error JSON, not exceptions
   - Missing resources should return {"error": "..."}, not crash

5. Update test expectations if needed:
   - Some tools may not be callable without full infrastructure
   - Mark those as xfail with clear reason

Goal: All 35+ tool invocations complete (pass or graceful fail), no crashes.
  </action>
  <verify>
```bash
cd C:/Users/Meow/Documents/Projects/megaraptor-mcp
pytest tests/integration/test_smoke_mcp_tools.py -v 2>&1 | tail -50
```
Should show test results for all tools, with passes and expected failures.
  </verify>
  <done>MCP tool smoke tests run without AttributeError, tools return responses or graceful errors</done>
</task>

</tasks>

<verification>
Run the full smoke test suite to verify gaps are closed:

```bash
cd C:/Users/Meow/Documents/Projects/megaraptor-mcp
pytest tests/integration/ -m smoke -v --tb=short 2>&1 | tail -80
```

Check the summary shows:
- VQL tests: still passing (regression check)
- Resource tests: still passing
- MCP tool tests: running (no AttributeError)
- Artifact tests: passing or properly marked
</verification>

<success_criteria>
1. test_generic_client_info passes or has documented xfail
2. test_generic_system_pslist passes or has documented xfail
3. All 35 MCP tools invocable (no AttributeError)
4. Smoke test pass rate improves from 45% to >70%
5. No regressions in previously passing tests (VQL, resources)
</success_criteria>

<output>
After completion, create `.planning/phases/02-smoke-tests/02-06-SUMMARY.md`
</output>
