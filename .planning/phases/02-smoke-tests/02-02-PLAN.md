---
phase: 02-smoke-tests
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - tests/integration/test_smoke_mcp_tools.py
autonomous: true

must_haves:
  truths:
    - "All 35 MCP tools can be invoked without raising exceptions"
    - "Tool responses are valid JSON with expected structure"
    - "Tools that return errors do so gracefully (error field, not exception)"
  artifacts:
    - path: "tests/integration/test_smoke_mcp_tools.py"
      provides: "Parametrized smoke tests for all 35 MCP tools"
      contains: "TOOL_SMOKE_INPUTS"
      min_lines: 100
  key_links:
    - from: "tests/integration/test_smoke_mcp_tools.py"
      to: "tests/integration/helpers/mcp_helpers.py"
      via: "import invoke_mcp_tool"
      pattern: "from tests.integration.helpers import"
    - from: "tests/integration/test_smoke_mcp_tools.py"
      to: "tests/integration/schemas"
      via: "import get_tool_schema"
      pattern: "from tests.integration.schemas import"
---

<objective>
Create parametrized smoke tests for all 35 MCP tools to verify basic callability and response structure.

Purpose: Validates SMOKE-01 (all tools callable) and SMOKE-05 (output validates against JSON Schema). Uses pytest.mark.parametrize to test all tools with minimal code duplication.

Output: Parametrized smoke test file covering all 35 MCP tools.
</objective>

<execution_context>
@C:\Users\Meow\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Meow\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-smoke-tests/02-RESEARCH.md
@.planning/phases/02-smoke-tests/02-01-SUMMARY.md
@src/megaraptor_mcp/tools/clients.py
@src/megaraptor_mcp/tools/artifacts.py
@src/megaraptor_mcp/tools/hunts.py
@src/megaraptor_mcp/tools/flows.py
@src/megaraptor_mcp/tools/vql.py
@src/megaraptor_mcp/tools/deployment.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Parametrized MCP Tool Smoke Tests</name>
  <files>tests/integration/test_smoke_mcp_tools.py</files>
  <action>
Create a parametrized smoke test that covers all 35 MCP tools. Following research guidance:
- Focus on callability, not functionality
- Use pytest-check for multiple assertions
- Validate JSON Schema where schemas exist
- Replace placeholders with real client IDs
- Mark tests appropriately for CI/CD filtering

The test should NOT wait for async operations to complete - that's for artifact smoke tests.

Create `tests/integration/test_smoke_mcp_tools.py`:
```python
"""Smoke tests for all 35 MCP tools.

Validates:
- SMOKE-01: All 35 MCP tools are callable and return non-error responses
- SMOKE-05: All tool outputs validate against JSON schemas for AI assistant parsing

These tests verify basic operability:
- Tool is callable
- Tool returns non-error response (or graceful error for invalid inputs)
- Response is valid JSON
- Response structure matches schema (where defined)

NOTE: These are callability tests, not functionality tests. They do NOT
wait for async operations (flows, hunts) to complete.
"""

import pytest
import json
from pytest_check import check
from jsonschema import validate, ValidationError

from tests.integration.helpers import invoke_mcp_tool, replace_placeholders
from tests.integration.schemas import get_tool_schema


# Define minimal valid inputs for each of 35 tools
# Format: (tool_name, arguments, expect_error)
# expect_error=True for tools that will fail with placeholder inputs
TOOL_SMOKE_INPUTS = [
    # =========================================================================
    # Client Management Tools (4 tools)
    # =========================================================================
    ("list_clients", {"limit": 10}, False),
    ("get_client_info", {"client_id": "C.placeholder"}, False),
    ("label_client", {
        "client_id": "C.placeholder",
        "labels": ["TEST-smoke-label"],
        "operation": "add"
    }, False),
    ("quarantine_client", {
        "client_id": "C.placeholder",
        "quarantine": False  # Use False to avoid actually quarantining
    }, False),

    # =========================================================================
    # Artifact Tools (3 tools)
    # =========================================================================
    ("list_artifacts", {"limit": 10}, False),
    ("get_artifact", {"artifact_name": "Generic.Client.Info"}, False),
    ("collect_artifact", {
        "client_id": "C.placeholder",
        "artifacts": ["Generic.Client.Info"],
        "timeout": 60
    }, False),

    # =========================================================================
    # Hunt Tools (4 tools)
    # =========================================================================
    ("create_hunt", {
        "artifacts": ["Generic.Client.Info"],
        "description": "TEST-smoke-hunt",
        "paused": True  # Keep paused for safety
    }, False),
    ("list_hunts", {"limit": 10}, False),
    ("get_hunt_results", {"hunt_id": "H.nonexistent"}, True),  # Will error - no such hunt
    ("modify_hunt", {"hunt_id": "H.nonexistent", "action": "pause"}, True),  # Will error

    # =========================================================================
    # Flow Tools (4 tools)
    # =========================================================================
    ("list_flows", {"client_id": "C.placeholder", "limit": 10}, False),
    ("get_flow_results", {
        "client_id": "C.placeholder",
        "flow_id": "F.nonexistent"
    }, False),  # May return empty, not error
    ("get_flow_status", {
        "client_id": "C.placeholder",
        "flow_id": "F.nonexistent"
    }, True),  # Will error - no such flow
    ("cancel_flow", {
        "client_id": "C.placeholder",
        "flow_id": "F.nonexistent"
    }, False),  # May succeed even for nonexistent

    # =========================================================================
    # VQL Tools (2 tools)
    # =========================================================================
    ("run_vql", {"query": "SELECT * FROM info()"}, False),
    ("vql_help", {"topic": "syntax"}, False),

    # =========================================================================
    # Deployment Tools - Server Deployment (6 tools)
    # =========================================================================
    # Note: Most deployment tools will error without actual deployment
    # We test that they're callable and return proper error structure
    ("deploy_server", {
        "deployment_type": "docker",
        "profile": "rapid",
        "server_hostname": "localhost"
    }, True),  # Will likely error without Docker setup
    ("deploy_server_docker", {
        "profile": "rapid",
        "server_hostname": "localhost"
    }, True),  # Will likely error
    ("deploy_server_cloud", {
        "cloud_provider": "aws",
        "profile": "standard"
    }, True),  # Will error - no cloud credentials
    ("get_deployment_status", {
        "deployment_id": "vr-nonexistent"
    }, True),  # Will error - no such deployment
    ("destroy_deployment", {
        "deployment_id": "vr-nonexistent",
        "confirm": False  # Don't actually destroy
    }, True),  # Will error - not confirmed
    ("list_deployments", {}, False),  # Should return empty list, not error

    # =========================================================================
    # Deployment Tools - Agent Deployment (7 tools)
    # =========================================================================
    ("generate_agent_installer", {
        "deployment_id": "vr-nonexistent",
        "os_type": "windows"
    }, True),  # Will error - no such deployment
    ("create_offline_collector", {
        "artifacts": ["Generic.Client.Info"],
        "target_os": "windows"
    }, True),  # May error without velociraptor binary
    ("generate_gpo_package", {
        "deployment_id": "vr-nonexistent"
    }, True),  # Will error - no such deployment
    ("generate_ansible_playbook", {
        "deployment_id": "vr-nonexistent"
    }, True),  # Will error - no such deployment
    ("deploy_agents_winrm", {
        "deployment_id": "vr-nonexistent",
        "targets": ["localhost"],
        "username": "test",
        "password": "test"
    }, True),  # Will error - no such deployment
    ("deploy_agents_ssh", {
        "deployment_id": "vr-nonexistent",
        "targets": ["localhost"],
        "username": "test"
    }, True),  # Will error - no such deployment
    ("check_agent_deployment", {
        "deployment_id": "vr-nonexistent"
    }, False),  # Should return client list, not error

    # =========================================================================
    # Deployment Tools - Configuration & Security (5 tools)
    # =========================================================================
    ("generate_server_config", {
        "deployment_id": "vr-nonexistent"
    }, True),  # Will error - no such deployment
    ("generate_api_credentials", {
        "deployment_id": "vr-nonexistent"
    }, True),  # Will error - no such deployment
    ("rotate_certificates", {
        "deployment_id": "vr-nonexistent"
    }, True),  # Will error - no such deployment
    ("validate_deployment", {
        "deployment_id": "vr-nonexistent"
    }, True),  # Will error - no such deployment
    ("export_deployment_docs", {
        "deployment_id": "vr-nonexistent"
    }, True),  # Will error - no such deployment
]


@pytest.mark.smoke
@pytest.mark.integration
@pytest.mark.parametrize("tool_name,arguments,expect_error", TOOL_SMOKE_INPUTS)
async def test_mcp_tool_callable(
    tool_name: str,
    arguments: dict,
    expect_error: bool,
    enrolled_client_id: str,
):
    """Smoke test: MCP tool is callable and returns valid structure.

    Validates SMOKE-01: All 35 MCP tools are callable and return non-error responses.
    Validates SMOKE-05: Tool outputs validate against JSON schemas.

    Args:
        tool_name: Name of the MCP tool to test
        arguments: Arguments to pass to the tool
        expect_error: Whether we expect this call to return an error
        enrolled_client_id: Real client ID from fixture
    """
    # Replace placeholder client IDs with real one
    test_args = replace_placeholders(arguments, client_id=enrolled_client_id)

    # Invoke the tool
    success, response = await invoke_mcp_tool(tool_name, test_args)

    # Basic response validation
    with check:
        assert response is not None, f"{tool_name}: returned None"

    if expect_error:
        # For tools expected to error, verify they return graceful error
        # (not crash with exception)
        if not success:
            with check:
                assert isinstance(response, str), \
                    f"{tool_name}: error should be string, got {type(response)}"
        # If success=True, that's also fine (tool handled gracefully)
    else:
        # For tools NOT expected to error
        if not success:
            # Some tools return error for placeholder inputs
            # This is acceptable as long as it's a structured error
            with check:
                assert isinstance(response, str), \
                    f"{tool_name}: error should be string"
        else:
            # Successful response - validate structure
            with check:
                assert response is not None, f"{tool_name}: empty success response"

            # Validate against schema if defined
            schema = get_tool_schema(tool_name)
            if schema and isinstance(response, (dict, list)):
                try:
                    validate(instance=response, schema=schema)
                except ValidationError as e:
                    # Don't fail on schema validation for smoke tests
                    # Just record it
                    with check:
                        assert False, \
                            f"{tool_name}: schema validation failed: {e.message}"


@pytest.mark.smoke
@pytest.mark.integration
async def test_tool_count_matches_expected():
    """Verify we're testing all 35 MCP tools."""
    expected_count = 35
    actual_count = len(TOOL_SMOKE_INPUTS)

    with check:
        assert actual_count == expected_count, \
            f"Expected {expected_count} tools, have {actual_count} in test inputs"

    # Verify no duplicates
    tool_names = [t[0] for t in TOOL_SMOKE_INPUTS]
    unique_names = set(tool_names)

    with check:
        assert len(unique_names) == len(tool_names), \
            f"Duplicate tool names found: {len(tool_names)} - {len(unique_names)}"
```
  </action>
  <verify>
Run: `pytest tests/integration/test_smoke_mcp_tools.py -v --collect-only 2>&1 | head -60`
Expected: Shows 36 test items collected (35 parametrized + 1 count test). No import errors.
  </verify>
  <done>
Parametrized smoke test exists covering all 35 MCP tools with SMOKE-01 and SMOKE-05 validation.
  </done>
</task>

<task type="auto">
  <name>Task 2: Register pytest smoke marker</name>
  <files>tests/conftest.py</files>
  <action>
Add the 'smoke' marker to pytest configuration if not already present.

Read the existing conftest.py and update the pytest_configure function to include the smoke marker:

```python
def pytest_configure(config):
    """Register custom markers."""
    config.addinivalue_line("markers", "unit: Unit tests (no external dependencies)")
    config.addinivalue_line("markers", "integration: Requires Docker infrastructure")
    config.addinivalue_line("markers", "slow: Long-running tests")
    config.addinivalue_line("markers", "smoke: Smoke tests for basic functionality verification")
```

Only add the smoke marker line if it's not already present.
  </action>
  <verify>
Run: `pytest --markers 2>&1 | grep smoke`
Expected: Shows "@pytest.mark.smoke: Smoke tests for basic functionality verification"
  </verify>
  <done>
pytest smoke marker is registered and available for test filtering.
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. Test collection works:
   ```bash
   pytest tests/integration/test_smoke_mcp_tools.py -v --collect-only 2>&1 | head -60
   ```

2. Smoke marker is registered:
   ```bash
   pytest --markers 2>&1 | grep smoke
   ```

3. Tests can run (will skip without Docker):
   ```bash
   pytest tests/integration/test_smoke_mcp_tools.py::test_tool_count_matches_expected -v --tb=short
   ```
</verification>

<success_criteria>
- [ ] tests/integration/test_smoke_mcp_tools.py exists
- [ ] TOOL_SMOKE_INPUTS contains exactly 35 tool entries
- [ ] Parametrized test covers all tools
- [ ] pytest smoke marker is registered
- [ ] Test collection shows 36 items (35 parametrized + 1 count test)
- [ ] All imports work without errors
</success_criteria>

<output>
After completion, create `.planning/phases/02-smoke-tests/02-02-SUMMARY.md`
</output>
