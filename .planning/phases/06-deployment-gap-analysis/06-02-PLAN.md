---
phase: 06-deployment-gap-analysis
plan: 02
type: execute
wave: 2
depends_on: ["06-01"]
files_modified:
  - tests/integration/test_investigation_workflow_e2e.py
autonomous: true

must_haves:
  truths:
    - "Full investigation workflow (triage to collection to analysis) completes successfully"
    - "VQL triage query returns process list from live deployment"
    - "Artifact collection schedules and completes on deployed server"
    - "Flow results are retrievable and contain expected data"
  artifacts:
    - path: "tests/integration/test_investigation_workflow_e2e.py"
      provides: "End-to-end investigation workflow test"
      contains: "test_full_investigation_workflow"
  key_links:
    - from: "tests/integration/test_investigation_workflow_e2e.py"
      to: "megaraptor_mcp/tools/vql.py"
      via: "VQL query execution"
      pattern: "run_vql|query"
    - from: "tests/integration/test_investigation_workflow_e2e.py"
      to: "megaraptor_mcp/tools/artifacts.py"
      via: "Artifact collection"
      pattern: "collect_artifact"
---

<objective>
Validate full DFIR investigation workflow works end-to-end against a deployed Velociraptor server.

Purpose: DEPLOY-03 requires proof that triage -> collect -> analyze workflow completes successfully. This validates the entire MCP toolchain works in a realistic deployment scenario.

Output: Integration test proving complete investigation workflow from triage through analysis.
</objective>

<execution_context>
@C:\Users\Meow\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Meow\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-deployment-gap-analysis/06-RESEARCH.md
@.planning/phases/06-deployment-gap-analysis/06-01-SUMMARY.md
@tests/conftest.py
@tests/integration/helpers/wait_helpers.py
@src/megaraptor_mcp/tools/vql.py
@src/megaraptor_mcp/tools/artifacts.py
@src/megaraptor_mcp/tools/flows.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create investigation workflow E2E test</name>
  <files>tests/integration/test_investigation_workflow_e2e.py</files>
  <action>
Create test_investigation_workflow_e2e.py implementing DEPLOY-03:

1. Test class `TestInvestigationWorkflowE2E`:
   - Use pytest markers: `@pytest.mark.integration`, `@pytest.mark.slow`
   - Require docker_compose_up and enrolled_client_id fixtures

2. `test_full_investigation_workflow()` (DEPLOY-03):

   Phase 1 - TRIAGE:
   - Execute VQL query: `SELECT Pid, Name, CommandLine FROM pslist()`
   - Use velociraptor_client.query() directly (not MCP tool for simplicity)
   - Assert query returns results with expected fields
   - Log triage summary: number of processes found

   Phase 2 - COLLECT:
   - Schedule artifact collection using MCP artifact tools or direct client
   - Use Generic.Client.Info artifact (quick, always available)
   - Get flow_id from collection result
   - Wait for flow completion using wait_for_flow_completion()
   - Timeout: 60 seconds

   Phase 3 - ANALYZE:
   - Retrieve flow results using client.get_flow_results() or MCP tool
   - Assert results contain expected client info fields
   - Assert result_count > 0
   - Log analysis summary: fields found, values

3. Error handling:
   - Clear error messages if any phase fails
   - Cleanup: cancel any running flows in finally block
   - Timeout handling with informative messages

4. Fixture dependencies:
   - Use docker_compose_up (from conftest.py)
   - Use enrolled_client_id (from conftest.py)
   - Use velociraptor_client (from conftest.py)

Note: This test uses the EXISTING test infrastructure (docker-compose.test.yml), NOT a new deployment. The deployment E2E tests in Plan 01 validate deployment creation. This test validates the investigation workflow against a running server.

Why use existing infrastructure:
- Faster test execution (no deployment overhead)
- More reliable (stable test environment)
- Matches how users would actually use MCP (connect to existing server)
  </action>
  <verify>
Run: pytest tests/integration/test_investigation_workflow_e2e.py -v --collect-only
Should show: test_full_investigation_workflow
  </verify>
  <done>
Test collected and ready for execution.
  </done>
</task>

<task type="auto">
  <name>Task 2: Execute investigation workflow test and verify DEPLOY-03</name>
  <files>tests/integration/test_investigation_workflow_e2e.py</files>
  <action>
Execute the investigation workflow test:

1. Ensure test infrastructure is running:
   - Check: docker ps | grep vr-test
   - If not running, tests will skip gracefully

2. Run the test:
   pytest tests/integration/test_investigation_workflow_e2e.py -v --tb=short -s

3. Verify output shows:
   - TRIAGE phase: Process list retrieved
   - COLLECT phase: Artifact collection scheduled and completed
   - ANALYZE phase: Results retrieved with expected data

4. If test fails:
   - Check docker-compose logs for server errors
   - Check enrolled client is available
   - Fix test code issues (do NOT modify production code)
   - Re-run until passing

5. If infrastructure unavailable:
   - Test should skip with clear message
   - This is acceptable behavior
  </action>
  <verify>
pytest tests/integration/test_investigation_workflow_e2e.py -v
Expected: 1 passed OR 1 skipped (if infrastructure unavailable)
  </verify>
  <done>
DEPLOY-03 requirement validated: Full investigation workflow (triage -> collect -> analyze) completes successfully.
  </done>
</task>

</tasks>

<verification>
1. `pytest tests/integration/test_investigation_workflow_e2e.py -v` - test passes or skips
2. Test output shows all three phases completed: triage, collect, analyze
3. VQL query returns process data
4. Artifact collection completes successfully
5. Flow results contain expected client information
</verification>

<success_criteria>
- test_investigation_workflow_e2e.py implements full workflow test
- Triage phase executes VQL and returns process list
- Collect phase schedules artifact and waits for completion
- Analyze phase retrieves and validates results
- DEPLOY-03 requirement validated with passing test
</success_criteria>

<output>
After completion, create `.planning/phases/06-deployment-gap-analysis/06-02-SUMMARY.md`
</output>
