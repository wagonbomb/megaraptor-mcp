---
phase: 03-error-handling
plan: 04
type: execute
wave: 3
depends_on: ["03-02", "03-03"]
files_modified:
  - src/megaraptor_mcp/tools/deployment.py
  - tests/integration/test_error_handling_comprehensive.py
autonomous: true

must_haves:
  truths:
    - "Deployment tools return clear errors for missing dependencies (ImportError)"
    - "Deployment not found returns 404-style error with hint"
    - "All 35 MCP tools handle error scenarios without exposing stack traces"
    - "Network timeouts return clear error messages"
    - "Authentication errors return clear messages without stack traces"
  artifacts:
    - path: "src/megaraptor_mcp/tools/deployment.py"
      provides: "Deployment tools with enhanced error handling"
      contains: "Deployment not found"
    - path: "tests/integration/test_error_handling_comprehensive.py"
      provides: "Comprehensive error handling test suite"
      min_lines: 100
  key_links:
    - from: "tests/integration/test_error_handling_comprehensive.py"
      to: "All MCP tools"
      via: "Error scenario testing"
      pattern: "test_.*_error"
---

<objective>
Enhance deployment.py error handling and create comprehensive error handling test suite validating all requirements (ERR-01 through ERR-07).

Purpose: Complete the error handling work with deployment tools and verify all phase requirements are met through comprehensive testing.

Output: Enhanced deployment.py with consistent error handling. Comprehensive test suite proving all error handling requirements are satisfied.
</objective>

<execution_context>
@C:\Users\Meow\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Meow\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-error-handling/03-RESEARCH.md
@.planning/phases/03-error-handling/03-02-SUMMARY.md
@.planning/phases/03-error-handling/03-03-SUMMARY.md
@src/megaraptor_mcp/tools/deployment.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Enhance deployment.py error handling consistency</name>
  <files>src/megaraptor_mcp/tools/deployment.py</files>
  <action>
deployment.py already has error handling but needs consistency improvements:

1. Review existing ImportError handlers - already good, keep pattern:
   ```python
   except ImportError as e:
       return [TextContent(type="text", text=json.dumps({
           "error": f"Missing dependency: {str(e)}",
           "suggestion": "Install required packages with: pip install megaraptor-mcp[deployment]"
       }, indent=2))]
   ```

2. Enhance "Deployment not found" errors throughout (get_deployment_status, destroy_deployment, etc.):
   - Add hint field: "Use 'list_deployments' to see available deployments"
   - Add resource_type: "deployment"
   - Add resource_id: deployment_id

3. Review all `except Exception as e` handlers:
   - Change from `"error": str(e)` to `"error": "Operation failed"` for generic cases
   - Or map specific exception types first
   - NEVER expose str(e) directly for unknown exceptions

4. Add validation for deployment_id format (should follow pattern like "vr-YYYYMMDD-XXXXXXXX"):
   ```python
   def validate_deployment_id(deployment_id: str) -> str:
       if not deployment_id:
           raise ValueError("deployment_id cannot be empty")
       if not deployment_id.startswith("vr-"):
           raise ValueError(
               f"Invalid deployment_id format: '{deployment_id}'. "
               "Expected format: 'vr-YYYYMMDD-XXXXXXXX'"
           )
       return deployment_id
   ```

5. For tools that take targets list (deploy_agents_winrm, deploy_agents_ssh):
   - Validate targets is non-empty
   - Validate each target is non-empty string

6. For deploy_server:
   - Validate deployment_type is one of: docker, binary, aws, azure (already done, enhance error)
   - Validate profile is one of: rapid, standard, enterprise
   - Validate ports are positive integers

7. Keep existing try/except structure but make messages consistent with other tools

Focus on making deployment tool errors match the pattern established in clients/artifacts/hunts/flows.
  </action>
  <verify>
    python -c "from megaraptor_mcp.tools.deployment import deploy_server, get_deployment_status, list_deployments; print('deployment.py OK')"
  </verify>
  <done>Deployment tools have consistent error handling matching other tool modules</done>
</task>

<task type="auto">
  <name>Task 2: Create comprehensive error handling test suite</name>
  <files>tests/integration/test_error_handling_comprehensive.py</files>
  <action>
Create comprehensive test suite that validates ALL phase 3 requirements:

```python
"""
Comprehensive error handling tests for Phase 3.

Tests validate requirements ERR-01 through ERR-07:
- ERR-01: Network timeout errors return clear messages
- ERR-02: VQL syntax errors provide correction hints
- ERR-03: Non-existent resources return 404-style errors
- ERR-04: Invalid parameters validated with clear messages
- ERR-05: Auth/permission errors handled gracefully
- ERR-06: No stack traces exposed
- ERR-07: Retry logic handles transient failures
"""
```

1. Test ERR-01 (Network timeouts):
   - `test_timeout_returns_clear_message()`: Mock gRPC DEADLINE_EXCEEDED, verify error message mentions "timeout" and has hint
   - `test_unavailable_returns_clear_message()`: Mock gRPC UNAVAILABLE, verify message mentions server availability

2. Test ERR-02 (VQL syntax hints):
   - `test_vql_semicolon_error()`: Run VQL with semicolon, assert error mentions "semicolon" and "VQL"
   - `test_vql_no_select_error()`: Run VQL without SELECT, assert error mentions "SELECT"
   - `test_vql_server_syntax_error_has_hint()`: Mock server returning syntax error, verify hint extracted

3. Test ERR-03 (404-style errors):
   - `test_client_not_found_404_style()`: get_client_info("C.nonexistent"), assert "not found", "hint", "resource_type"
   - `test_artifact_not_found_404_style()`: get_artifact("Nonexistent.Artifact"), assert 404 pattern
   - `test_flow_not_found_404_style()`: get_flow_status with bad flow_id, assert 404 pattern
   - `test_hunt_not_found_returns_gracefully()`: get_hunt_results with bad hunt_id

4. Test ERR-04 (Invalid parameter validation):
   - `test_negative_limit_rejected()`: list_clients(limit=-1), assert validation error
   - `test_empty_client_id_rejected()`: get_client_info(""), assert validation error
   - `test_invalid_client_id_format_rejected()`: get_client_info("not-valid"), assert mentions "C."
   - `test_invalid_hunt_id_format_rejected()`: get_hunt_results("not-valid"), assert mentions "H."
   - `test_empty_vql_rejected()`: run_vql(""), assert validation error

5. Test ERR-05 (Auth errors):
   - `test_unauthenticated_error_handled()`: Mock UNAUTHENTICATED, verify clear message about credentials
   - `test_permission_denied_handled()`: Mock PERMISSION_DENIED, verify message about permissions

6. Test ERR-06 (No stack traces):
   - `test_no_stack_traces_in_errors()`: For each tool with forced error, assert response doesn't contain:
     - "Traceback"
     - "File \""
     - ".py\", line"
     - Common Python error patterns
   - Create parametrized test across all error scenarios

7. Test ERR-07 (Retry logic):
   - `test_retry_on_unavailable()`: Mock UNAVAILABLE then success, verify retry worked
   - `test_no_retry_on_not_found()`: Mock NOT_FOUND, verify no retry (fails immediately)
   - `test_no_retry_on_auth_error()`: Mock UNAUTHENTICATED, verify no retry

Use pytest fixtures to mock grpc.RpcError with specific StatusCodes.
Use pytest.mark.parametrize for stack trace checking across tools.
Mark integration tests appropriately.
  </action>
  <verify>
    cd "C:\Users\Meow\Documents\Projects\megaraptor-mcp" && python -m pytest tests/integration/test_error_handling_comprehensive.py -v --timeout=120
  </verify>
  <done>Comprehensive tests validate all ERR-01 through ERR-07 requirements</done>
</task>

<task type="auto">
  <name>Task 3: Verify all smoke tests still pass with error handling</name>
  <files>tests/smoke/</files>
  <action>
Run existing smoke tests to ensure error handling additions don't break normal operation:

1. Run all smoke tests:
   ```
   pytest tests/smoke/ -v --timeout=120
   ```

2. If any failures:
   - Check if error handling code path is being triggered incorrectly
   - Verify validation isn't too strict (e.g., rejecting valid inputs)
   - Fix any regressions

3. Document any changes needed to smoke test inputs due to stricter validation

This task is verification-only. If smoke tests pass, no code changes needed.
If smoke tests fail, fix the error handling code to not break valid use cases.
  </action>
  <verify>
    cd "C:\Users\Meow\Documents\Projects\megaraptor-mcp" && python -m pytest tests/smoke/ -v --timeout=180
  </verify>
  <done>All existing smoke tests pass with new error handling</done>
</task>

</tasks>

<verification>
1. All imports work for deployment.py
2. Deployment errors follow consistent pattern
3. Comprehensive test suite passes
4. All smoke tests still pass
5. No stack traces in any error response
6. Requirements ERR-01 through ERR-07 validated by tests
</verification>

<success_criteria>
- deployment.py has consistent error handling matching other tools
- Comprehensive test suite covers all 7 error handling requirements
- Tests prove: timeouts handled, VQL hints work, 404 errors, validation errors, auth errors, no stack traces, retry logic
- All existing smoke tests pass (no regressions)
- Phase 3 complete: All MCP tools handle failure scenarios gracefully
</success_criteria>

<output>
After completion, create `.planning/phases/03-error-handling/03-04-SUMMARY.md`
</output>
